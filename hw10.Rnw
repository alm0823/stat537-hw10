\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{float}
\usepackage{amsmath}

\usepackage{enumitem}
\setlist{parsep=5.5pt}

\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{Stat 537 Homework 10}
\chead{April 28, 2016}
\rhead{Andrea Mack and Kenny Flagg}
\setlength{\headheight}{18pt}
\setlength{\headsep}{2pt}

\title{Stat 537 Homework 10}
\author{Andrea Mack and Kenny Flagg}
\date{April 28, 2016}

<<setup, echo=FALSE, message=FALSE, cache=FALSE>>=
require(knitr)
opts_chunk$set(echo = FALSE, comment = NA, width = 80,
               fig.align = 'center', fig.width = 6, fig.height = 3,
               size = 'footnotesize',
               dev = 'pdf', dev.args = list(pointsize = 8))
knit_theme$set('print')

require(xtable)
require(psych)
require(lavaan)
require(polycor)
require(psych)
require(semPlot)
require(effects)
require(partykit)
require(openintro)
require(rpart)
@

\begin{document}

\maketitle

{\it {\bf Part 1}: Classification Trees for the diagnosis of Mild Osteoarthritis using genetic information.}

{\it In Marshall et al. \verb|http://www.sciencedirect.com/science/article/pii/S1063458405001524|, they use logistic regression models to build and then validate (to some degree) a predictor for osteoarthritis/not using a set of genetic markers. Read the paper to answer the following questions.}

{\it Read the paper, focusing on the statistical aspects of their work.}

\begin{enumerate}

\item %1
{\it Fit their top ranked model from Supplemental Table 5. Make effects plots for the resulting estimated model using the following code.}

<<one>>=
marsh1 <- read.csv("marshalcomb.csv",header=T)
#View(marsh1)
marsh1$D2 <- as.factor(marsh1$D2)
marsh1$Ethnicity <- as.factor(marsh1$Ethnicity)
marsh1R<-marsh1[,-c(1,2,3,4,5,6,8)]
@

<<one.plot,fig.height=12,fig.width=9,out.height='8in',out.width='6in'>>=
#Fit the model and call it glm1
#Top row in the excel file
glm1 <- glm(D2 ~ G2AN+IKBKAP+IL13RA1+LAMC1+MAFB+PF4+TNFAIP6, data=marsh1R, family=binomial)
summary(glm1)
plot(allEffects(glm1),type="link",rows=4,cols=2)
@

\pagebreak
\item %2
{\it They use logit values above or below 0 to predict whether an observation is diseased or not (below equation 1). Explain that choice.}

When the probability of getting the disease is the same as not getting the disease( p=0.5), the logit is equal to 0. So a logit $<$ zero is when there is a higher probability of not getting a disease, and when there is a higher probability of getting the disease than not getting the disease, logit is $>$ zero.

So if the probability of getting a disease is higher than half, logit is positive and we assign a subject to getting the disease. If the probability of getting a disease is less than half, logit is negative we assign a subject to not getting the disease.

\item %3
{\it In the section on page 865 ``Reference data set (AD1F2) for the best gene combinations'' describe the type of modeling/model selection they are considering. Supplement table 5 contains more details of the results of this process.}

The statistician fit a model for each possible combination of predictor genes. He then made a ROC for each model, and computed the AUC. An AUC closer to 1 indicates that the model does a good job of correctly predicting mild OA and minimizes the misdiagnosis of mild OA.

The models with AUCs above 0.92, which would be classified as having ``Outstanding Discrimination'', are bolded in the Supplemental Table 5, and the model with the AUC closest to 1 is chosen. Two models were tied for the highest AUC. The top model listed has fewer predictors than the second model (same AUC for the ROC), and we believe that is why it is chosen to be the top model.

\item %4
{\it Describe what they are doing in the section ``Prospective (Blind) test'' starting on page 864. Use terminology from JWHT.}

They had an ensemble of 68 models that had an AUC of the ROC $>$ 0.9. For each of the models, a prediction was made to classify a person as mild OA or control. If a person is classified as mild OA, a 1 is assigned, if a person is classified as control, a -1 is assigned for each model. The sum of the 68 classifications iare calculated and the majority vote from all models determines whether a person's class is mild OA or control.

\item %5
{\it Fit a classification tree using rpart using all 9 of the genetic variables listed below using {\texttt minbucket}=4 and {\texttt cp}=0.000000001 as in the code below. Prune the tree using the Min CV rule after consulting multiple calls to \texttt{ printcp()} and {\texttt plotcp()}. Do not load mvpart before doing this problem. Discuss the results of your CV process and how you chose your tree size.}

The results of our CV process resulted in a tree with two terminal nodes, with {\texttt PF4} being the most important variable in predicting disease presence. Too much weight should not be put on the particuar split chosen on {\texttt PF4} in terms of importance of predicting disease because if the tree was created again, another split or another variable to split on may have been chosen.

We chose the tree size based on the min CV rule. Since the size of tree with the min CV changes each time a tree is generated, it is not reasonable to base the size of the tree decision on the results/output from one tree. Below we plotted the output from one tree, but also summarized the proportion of trees with min CV at each size after fitting 1000 different trees. Most often, a tree of size 2 resulted in the lowest {\texttt xerror}, therefore we decided to use a tree of size 2. (I think I made a tree of size 4...?) We used the min {\texttt cp} based on a tree with 2 splits from the first tree generated, but could have used the similarly found {\texttt cp} from another tree.

<<five, cache=TRUE, results='hide'>>=
set.seed(112)

# You can also see the Min CV choice in repeated calls using:
ntrees <- 1000
ret <- rep(NA, ntrees)
for (i in 1:ntrees){
  tree1 <- rpart(factor(D2)~.,data=marsh1R,cp=0.000000001,minbucket=4)
  a <- printcp(tree1)
  tree2 <- rpart(factor(D2)~.,data=marsh1R,cp=0.000000001,minbucket=4)
  b <- printcp(tree1)
  # Get size of smallest tree with the minimum CV error
  # Tree size is number of splits + 1
  ret[i] <- min(a[which(a[, "xerror"] == min(a[, "xerror"])), "nsplit"])+1
}
@
<<five.plot,>>=
par(mfrow = c(1, 2), mar = c(5.1, 4.1, 7.1, 2.1))
plotcp(tree1, main = "CV Error vs cp for One Tree\n\n\n")
plot(prop.table(table(ret)), lwd = 10, lend = 1, xaxt = 'n',
     main = paste("Distribution of the Min-CV Optimal\nTree Size for",
                  ntrees, "Trees"),
     ylab = "Proportion", xlab = "Size of Tree",
     xlim = c(0.5, max(ret)+0.5), ylim = c(0, 0.5))
text(x = sort(unique(ret)), y = prop.table(table(ret)),
     labels = signif(prop.table(table(ret)), 3), pos = 3)
axis(1, at = 1:max(ret))
@


<<mincp>>=
#min.cp <- order(a[, "xerror"])
mincp2 <- 0.1
prune2 <- prune(tree1, cp=mincp2)
plot(as.party(prune2))

@
<<mvpart, include = FALSE>>=
#require(mvpart)

@

\item %6
{\it Based on these results, choose two different sized trees that could be reasonable based on the different CV selections. Report a plot of your two pruned trees using the partykit's \verb|plot(as.party(PRUNEDTREENAME))|. Discuss the differences in the two trees.}

Based on the plot showing the rate of min CP occuring for given sizes in 1000 trials, it appears trees of size 6 or 8 had the minimum {\texttt xerror} quite often as well, so we will consider trees of size 6 and 8 in this problem.

The difference in the two trees is that the effect of {\texttt PF4} at or above 6.373 on disease prediction depends on {\texttt IL13RA1} and {\texttt MAFB} when the tree size is 8, which is not shown in the tree pruned to size 6. The variables and splits used for {\texttt PF4} less than 6.373 are the same for trees of sizes 6 and 8.


<<six>>=
mincp6 <- 0.000000002
prune6 <- prune(tree1, cp=mincp6)
plot(as.party(prune6))

mincp8 <- 0.028
prune8 <- prune(tree1, cp=mincp8)
plot(as.party(prune8))

@

\item %7
{\it Adjust the following code as needed to make a plot of the predictions from the three models. Use these results to discuss the differences in these approaches/results. I had models called tree1p, tree2p, and glm1.}

WHY DOES MARK HAVE A GLM OBJECT AND GLM CODE IN HERE? This makes me think we should have a glm object as well. Any thoughts?

We are fitting a binomial model, and so the measures on the side panels indicate the predicted probability of disease. As the size increases, the distributions of the probabilities of disease have a more continuous look, versus the binary for size 2. As the size increases, the probability of disease is less separated. For example, for size 2, we see two groupings of probabilities of disease, at 0 and at (or near) 1. For size 6, we see the six groups clustered into two, but the clusters are closer together than with size 2. One cluster (no disease) is from 0 to around 0.3 and the other is around 0.8 to 1 (disease). Again with size 8, we see two main clusters of the eight groups of predicted probabilities, but the highest probability in the no disease cluster is around 0.4 while the lowest probability in the disease cluster is around 0.7.

In summary, as size increases, for binomial outcomes, the range of predicted probabilities increases, and the distance between the min predicted probability of success and the max predicted probability of failure decreases.

-$>$ Possibly disucess in terms of ``decrease in purity".

<<seven>>=
fits<-data.frame(Size2=predict(prune2)[,2], Size6=predict(prune6)[,2],Size8=predict(prune8)[,2])
plot(fits)
@

\item %8
{\it The predictor variables were log10-transformed. Generally discuss how this impacts the tree-based approach vs the GLM approach. You can either undo their transformations and refit models or just discuss the impacts based on thinking about how the models work and why they might have done this transformation initially.}

The tree based approach is {\it not} invariant to changes in the scale of the predictors. Therefore, when the log10 transformation is done on the predictor variables, the resulting trees will also change.

The glm approach produces estimates that are invariant to log10 transformations of the predictor variables, athough the se(estimates) may change depending on the validity of the assumptions.

-$>$ Kenny, could you verify?


{\bf Part 2 :} {\it Use the following code to identify an optimal predictive model for first year college GPAs. The following code will split out half the observations into a training data set and fit two different conditional inference trees using all available predictors. It also fits a recursive partitioning tree. }

<<ten>>=
require(openintro)
data(satGPA)

set.seed(123456) #So that you can repeat the running of the code and get the same results
train=sample(1:1000,size=500)
ctree1<-ctree(FYGPA~.,data=satGPA[train,],mincriterion=0.95)
ctree2<-ctree(FYGPA~.,data=satGPA[train,],mincriterion=0.9)
rpart_full<-rpart(FYGPA~.,data=satGPA[train,],cp=0.00000001)
printcp(rpart_full)
par(mfrow=c(1,1))
plotcp(rpart_full)
@

\item %9
{\it ) Prune the tree using the 1SE and Min CV rules (either from a single 10-fold CV run or select a tree size for each as a consensus that you build from multiple CV runs). }

<<next>>=
#Do your own pruning and put results into rpart21SE and rpart2MinCV
predict(ctree1,newdata=satGPA[-train,])
predict(ctree2,newdata=satGPA[-train,])
predict(rpart21SE,newdata=satGPA[-train,])
predict(rpart2MinCV,newdata=satGPA[-train,])
@

\item %10
{\it Calculate and compare the validation error for the 4 models using the withheld responses and discuss the choice of significance threshold and CV rule.}

\end{enumerate}

\pagebreak
\section*{R Code Appendix}

\subsection*{Problem 1}

<<one, echo=TRUE, eval=FALSE>>=
@
<<one.plot, echo=TRUE, eval=FALSE>>=
@


\subsection*{Problem 4}

\subsection*{Problem 5}

<<five, echo=TRUE, eval=FALSE>>=
@
<<five.plot, echo=TRUE, eval=FALSE>>=
@

\subsection*{Problem 6}

\subsection*{Problem 7}

<<seven, echo=TRUE, eval=FALSE>>=
@

\subsection*{Problem 8}

\subsection*{Problem 9}

\subsection*{Problem 10}

<<ten, echo=TRUE, eval=FALSE>>=
@

\end{document}
