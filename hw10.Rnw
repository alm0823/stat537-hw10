\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{float}
\usepackage{amsmath}

\usepackage{enumitem}
\setlist{parsep=5.5pt}

\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{Stat 537 Homework 10}
\chead{April 28, 2016}
\rhead{Andrea Mack and Kenny Flagg}
\setlength{\headheight}{18pt}
\setlength{\headsep}{2pt}

\title{Stat 537 Homework 10}
\author{Andrea Mack and Kenny Flagg}
\date{April 28, 2016}

<<setup, echo=FALSE, message=FALSE, cache=FALSE>>=
require(knitr)
opts_chunk$set(echo = FALSE, comment = NA, width = 80,
               fig.align = 'center', fig.width = 6, fig.height = 3,
               size = 'footnotesize',
               dev = 'pdf', dev.args = list(pointsize = 8))
knit_theme$set('print')

require(xtable)
require(psych)
require(lavaan)
require(polycor)
require(psych)
require(semPlot)
require(effects)
require(partykit)
require(openintro)
require(rpart)
require(arm)

data(satGPA)
@

\begin{document}

\maketitle

{\it {\bf Part 1}: Classification Trees for the diagnosis of Mild Osteoarthritis using genetic information.}

{\it In Marshall et al. \verb|http://www.sciencedirect.com/science/article/pii/S1063458405001524|, they use logistic regression models to build and then validate (to some degree) a predictor for osteoarthritis/not using a set of genetic markers. Read the paper to answer the following questions.}

{\it Read the paper, focusing on the statistical aspects of their work.}

\begin{enumerate}

\item %1
{\it Fit their top ranked model from Supplemental Table 5. Make effects plots for the resulting estimated model using the following code.}

<<one>>=
marsh1 <- read.csv("marshalcomb.csv",header=T)
#View(marsh1)
marsh1$D2 <- as.factor(marsh1$D2)
marsh1$Ethnicity <- as.factor(marsh1$Ethnicity)
marsh1R<-marsh1[,-c(1,2,3,4,5,6,8)]
@

<<one.plot,fig.height=12,fig.width=9,out.height='8in',out.width='6in'>>=
#Fit the model and call it glm1
#Top row in the excel file
glm1 <- glm(D2 ~ G2AN+IKBKAP+IL13RA1+LAMC1+MAFB+PF4+TNFAIP6, data=marsh1R, family=binomial)
summary(glm1)
plot(allEffects(glm1),type="link",rows=4,cols=2)
@

\pagebreak
\item %2
{\it They use logit values above or below 0 to predict whether an observation is diseased or not (below equation 1). Explain that choice.}

When the probability of getting the disease is the same as not getting the disease ($p=0.5$), the logit is equal to 0. So a logit $<$ zero is when there is a higher probability of not getting a disease, and when there is a higher probability of getting the disease than not getting the disease, logit is $>$ zero.

So if the probability of getting a disease is higher than half, logit is positive and we assign a subject to getting the disease. If the probability of getting a disease is less than half, logit is negative we assign a subject to not getting the disease.

\item %3
{\it In the section on page 865 ``Reference data set (AD1F2) for the best gene combinations'' describe the type of modeling/model selection they are considering. Supplement table 5 contains more details of the results of this process.}

The statistician fit a model for each possible combination of predictor genes. He then made a ROC for each model, and computed the AUC. An AUC closer to 1 indicates that the model does a good job of correctly predicting mild OA and minimizes the misdiagnosis of mild OA.

The models with AUCs above 0.92, which would be classified as having ``Outstanding Discrimination'', are bolded in the Supplemental Table 5, and the model with the AUC closest to 1 is chosen. Two models were tied for the highest AUC. The top model listed has fewer predictors than the second model (same AUC for the ROC), and we believe that is why it is chosen to be the top model.

\item %4
{\it Describe what they are doing in the section ``Prospective (Blind) test'' starting on page 864. Use terminology from JWHT.}

They had an ensemble of 68 models that had an AUC of the ROC $>$ 0.9. For each of the models, a prediction was made to classify a person as mild OA or control. If a person is classified as mild OA, a 1 is assigned, if a person is classified as control, a -1 is assigned for each model. The sum of the 68 classifications are calculated and the majority vote from all models determines whether a person's class is mild OA or control.

\item %5
{\it Fit a classification tree using rpart using all 9 of the genetic variables listed below using {\texttt minbucket}=4 and {\texttt cp}=0.000000001 as in the code below. Prune the tree using the Min CV rule after consulting multiple calls to \texttt{printcp()} and {\texttt plotcp()}. Do not load mvpart before doing this problem. Discuss the results of your CV process and how you chose your tree size.}

The results of our CV process resulted in a tree with two terminal nodes, with {\texttt PF4} being the most important variable in predicting disease presence. Too much weight should not be put on the particular split chosen on {\texttt PF4} in terms of importance of predicting disease because if the tree was created again, another split or another variable to split on may have been chosen.

We chose the tree size based on the min CV rule. Since the size of tree with the min CV changes each time a tree is generated, it is not reasonable to base the size of the tree decision on the results/output from one tree. Below we plotted the output from one tree, but also summarized the proportion of trees with min CV at each size after fitting 1000 different trees. Most often, a tree of size 2 resulted in the lowest {\texttt xerror}, therefore we decided to use a tree of size 2. We used the min {\texttt cp} based on a tree with 2 splits from the first tree generated, but could have used the similarly found {\texttt cp} from another tree.

<<five, cache=TRUE, results='hide'>>=
set.seed(112)

# You can also see the Min CV choice in repeated calls using:
ntrees <- 1000
ret <- rep(NA, ntrees)
for (i in 1:ntrees){
  tree1 <- rpart(factor(D2)~.,data=marsh1R,cp=0.000000001,minbucket=4)
  a <- printcp(tree1)
  tree2 <- rpart(factor(D2)~.,data=marsh1R,cp=0.000000001,minbucket=4)
  b <- printcp(tree1)
  # Get size of smallest tree with the minimum CV error
  # Tree size is number of splits + 1
  ret[i] <- min(a[which(a[, "xerror"] == min(a[, "xerror"])), "nsplit"])+1
}
@
<<five.plot,>>=
par(mfrow = c(1, 2), mar = c(5.1, 4.1, 7.1, 2.1))
plotcp(tree1, main = "CV Error vs cp for One Tree\n\n\n")
plot(prop.table(table(ret)), lwd = 10, lend = 1, xaxt = 'n',
     main = paste("Distribution of the Min-CV Optimal\nTree Size for",
                  ntrees, "Trees"),
     ylab = "Proportion", xlab = "Size of Tree",
     xlim = c(0.5, max(ret)+0.5), ylim = c(0, 0.5))
text(x = sort(unique(ret)), y = prop.table(table(ret)),
     labels = signif(prop.table(table(ret)), 3), pos = 3)
axis(1, at = 1:max(ret))
@


<<mincp>>=
min.cp <- a[order(a[, "xerror"])[1], "CP"] # Based on last tree fit, gives a tree with 6 nodes
mincp2 <- 0.1
prune2 <- prune(tree1, cp=mincp2)
plot(as.party(prune2))
@
<<mvpart, include = FALSE>>=
#require(mvpart)

@

\item %6
{\it Based on these results, choose two different sized trees that could be reasonable based on the different CV selections. Report a plot of your two pruned trees using the partykit's \verb|plot(as.party(PRUNEDTREENAME))|. Discuss the differences in the two trees.}

Based on the plot showing the rate of min CP occurring for given sizes in 1000 trials, it appears trees of size 6 or 8 had the minimum {\texttt xerror} quite often as well, so we will consider trees of size 6 and 8 in this problem.

The difference in the two trees is that the effect of {\texttt PF4} at or above 6.373 on disease prediction depends on {\texttt IL13RA1} and {\texttt MAFB} when the tree size is 8, which is not shown in the tree pruned to size 6. The variables and splits used for {\texttt PF4} less than 6.373 are the same for trees of sizes 6 and 8.


<<six>>=
mincp6 <- 0.028
prune6 <- prune(tree1, cp=mincp6)
plot(as.party(prune6))

mincp8 <- 0.000000002
prune8 <- prune(tree1, cp=mincp8)
plot(as.party(prune8))

@

\item %7
{\it Adjust the following code as needed to make a plot of the predictions from the three models. Use these results to discuss the differences in these approaches/results. I had models called tree1p, tree2p, and glm1.}

%WHY DOES MARK HAVE A GLM OBJECT AND GLM CODE IN HERE? This makes me think we should have a glm object as well. Any thoughts?
% I think he's trying to illustrate a parallel since we're predicting binary responses...?

We are fitting a binomial model, and so the measures on the side panels indicate the predicted probability of disease. As the size increases, the distributions of the probabilities of disease have a more continuous look, versus the binary for size 2. As the size increases, the probability of disease is less separated. As a side note, another way to think of the ``separation" discussed is in terms of ``purity", which the text uses. More separation would be more pure. For example, for size 2, we see two groupings of probabilities of disease, at 0 and at (or near) 1. For size 6, we see the six groups clustered into two, but the clusters are closer together than with size 2. One cluster (no disease) is from 0 to around 0.3 and the other is around 0.8 to 1 (disease). Again with size 8, we see two main clusters of the eight groups of predicted probabilities, but the highest probability in the no disease cluster is around 0.4 while the lowest probability in the disease cluster is around 0.7.

In summary, as size increases, for binomial outcomes, the range of predicted probabilities increases, and the distance between the min predicted probability of success and the max predicted probability of failure decreases.

%-$>$ Possibly disucess in terms of ``decrease in purity".

<<seven,fig.height=6>>=
fits<-data.frame(Size2=jitter(predict(prune2)[,2]), Size6=jitter(predict(prune6)[,2]),Size8=jitter(predict(prune8)[,2]))
plot(fits)
@

\item %8
{\it The predictor variables were log10-transformed. Generally discuss how this impacts the tree-based approach vs the GLM approach. You can either undo their transformations and refit models or just discuss the impacts based on thinking about how the models work and why they might have done this transformation initially.}

<<try.it, cache=TRUE, include=FALSE>>=
ex <- 10^(marsh1R[,-1])
marsh1R.ex<-data.frame(cbind(D2=marsh1R[,1], ex))


set.seed(112)

# You can also see the Min CV choice in repeated calls using:
ntrees.ex <- 1000
ret.ex <- rep(NA, ntrees.ex)
for (i in 1:ntrees.ex){
  tree1.ex <- rpart(factor(D2)~.,data=marsh1R.ex,cp=0.000000001,minbucket=4)
  a.ex <- printcp(tree1.ex)
    # Get size of smallest tree with the minimum CV error
  # Tree size is number of splits + 1
  ret.ex[i] <- min(a.ex[which(a.ex[, "xerror"] == min(a.ex[, "xerror"])), "nsplit"])+1
}
@

<<try2>>=

par(mfrow = c(1, 2), mar = c(5.1, 4.1, 7.1, 2.1))
plotcp(tree1.ex, main = "CV Error vs cp for One Tree\n\n\n")
plot(prop.table(table(ret.ex)), lwd = 10, lend = 1, xaxt = 'n',
     main = paste("Distribution of the Min-CV Optimal\nTree Size for",
                  ntrees.ex, "Trees"),
     ylab = "Proportion", xlab = "Size of Tree",
     xlim = c(0.5, max(ret.ex)+0.5), ylim = c(0, 0.5))
text(x = sort(unique(ret.ex)), y = prop.table(table(ret.ex)),
     labels = signif(prop.table(table(ret.ex)), 3), pos = 3)
axis(1, at = 1:max(ret.ex))


@

The tree based approach is invariant to changes in the scale of the predictors (however, this is not true with conditional pruning). Therefore, when the log10 transformation is done on the predictor variables, the resulting trees, and therefore classification or prediction did not change much. We did 1000 more simulations using the backtransformed predictors, and the distribution of optimal tree size was very similar to when using the transformed predictors. While a formal test was not done, the proportions are similar enough to suggest the differences observed may be due to random chance.

It makes sense that the tree approach is invariant to the log10 transformation because the log10 transformation is monotone. The tree finds splits by searching along the range of the predictor and looking for the split that is the most pure. It does not matter what scale the predictor is on because the purest split on any scale will partition the observations in the same way.

<<glm.try>>=
glm1.ex <- glm(as.factor(as.numeric(D2)-1) ~ G2AN+IKBKAP+IL13RA1+LAMC1+MAFB+PF4+TNFAIP6, data=marsh1R.ex, family=binomial)
#summary(glm1.ex)
#plot(allEffects(glm1.ex),type="link",rows=4,cols=2)

par(mfrow=c(1,1))
plot(invlogit(predict(glm1)), invlogit(predict(glm1.ex)), xlab = "LOG10(x) Predictions", ylab = "x Predictions")
abline(a=0,b=1)

@

The predicted probability of disease changes depending on whether the explanatory variables are log10 transformed or not because the points do not lie on the line. This is opposite of what we thought was true.


{\bf Part 2 :} {\it Use the following code to identify an optimal predictive model for first year college GPAs. The following code will split out half the observations into a training data set and fit two different conditional inference trees using all available predictors. It also fits a recursive partitioning tree. }

<<ten, include = FALSE>>=
set.seed(123456) #So that you can repeat the running of the code and get the same results
train=sample(1:1000,size=500)
ctree1<-ctree(FYGPA~.,data=satGPA[train,],mincriterion=0.95)
ctree2<-ctree(FYGPA~.,data=satGPA[train,],mincriterion=0.9)

#set.seed(111)
#split.data.frame(satGPA, size = dim(satGPA)[1])


plot(ctree1)
plot(ctree2)

#table(predict(ctree1),satGPA[train,]$FYGPA)
@


\item %9
{\it Prune the tree using the 1SE and Min CV rules (either from a single 10-fold CV run or select a tree size for each as a consensus that you build from multiple CV runs). }

<<sat, include = FALSE>>=
summary(ctree1)
rpart_full<-rpart(FYGPA~.,data=satGPA[train,],cp=0.00000001)
printcp(rpart_full)
par(mfrow=c(1,1))
plotcp(rpart_full)

mincp.full <- min(printcp(rpart_full)[,"xerror"])
#1se says size 4
se1.full <- 0.0179

full1 <- prune(rpart_full, cp = 0.01)
full2 <- prune(rpart_full, cp = 0.02)

@

<<plots>>=

plotcp(rpart_full)
plot(as.party(full1))
plot(as.party(full2))


@


\item %10
{\it Calculate and compare the validation error for the 4 models using the withheld responses and discuss the choice of significance threshold and CV rule.}

<<valid, results = 'asis'>>=
pred.ctree1 <- predict(ctree1, newdata=satGPA[-train,])
pred.ctree2 <- predict(ctree2, newdata=satGPA[-train,])
pred.full1 <- predict(full1, newdata=satGPA[-train,])
pred.full2 <- predict(full2, newdata=satGPA[-train,])

mc1 <- mean((satGPA[-train,]$FYGPA-pred.ctree1)^2)
mc2 <- mean((satGPA[-train,]$FYGPA-pred.ctree2)^2)
mf1 <- mean((satGPA[-train,]$FYGPA-pred.full1)^2)
mf2 <- mean((satGPA[-train,]$FYGPA-pred.full2)^2)

vect <- round(c(mc1,mc2,mf1,mf2),3)

names.vect <- c("csig0.95", "csig0.9", "mincv", "1se")

table <- rbind(names.vect, vect)
rownames(table) <- c("Pruning Method", "CV Error")

print(xtable(table), include.colnames = FALSE)
@

The CV errors are all quite similar, although since the CV error is the smallest using the minCV rule for pruning, this appears to be the best. Next best is the conditional tree using the 0.9 significance threshold, followed by the conditional tree using the 0.95 significance threshold, and the tree found using the 1se pruning rule did the worst in terms of CV error.

When using {\texttt ctree} there is an option to set the significance threshold for the permutation test to see if the next split does better than a random split. The tree stops when the test results in a decision of failing to reject that the next split is a random split. In the table, under {\texttt csig0.9}, we see a lower CV error than under {\texttt csig0.95} because with the lower significance threshold, we reject more often, meaning that the tree will be larger. So in this case, the larger tree did slightly better in terms of CV error than the smaller tree, but not by much.
\end{enumerate}

\pagebreak
\section*{R Code Appendix}

\subsection*{Problem 1}

<<one, echo=TRUE, eval=FALSE>>=
@
<<one.plot, echo=TRUE, eval=FALSE>>=
@

\subsection*{Problem 5}

<<five, echo=TRUE, eval=FALSE>>=
@
<<five.plot, echo=TRUE, eval=FALSE>>=
@
<<mincp, echo=TRUE, eval=FALSE>>=
@

\subsection*{Problem 6}

<<six, echo=TRUE, eval=FALSE>>=
@

\subsection*{Problem 7}

<<seven, echo=TRUE, eval=FALSE>>=
@

\subsection*{Problem 8}

<<try.it, echo=TRUE, eval=FALSE>>=
@
<<try2, echo=TRUE, eval=FALSE>>=
@
<<glm.try, echo=TRUE, eval=FALSE>>=
@

\subsection*{Problem 9}

<<ten, echo=TRUE, eval=FALSE>>=
@
<<sat, echo=TRUE, eval=FALSE>>=
@
<<plots, echo=TRUE, eval=FALSE>>=
@

\subsection*{Problem 10}

<<valid, echo=TRUE, eval=FALSE>>=
@

\end{document}
