\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{float}
\usepackage{amsmath}

\usepackage{enumitem}
\setlist{parsep=5.5pt}

\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{Stat 537 Homework 10}
\chead{April 28, 2016}
\rhead{Andrea Mack and Kenny Flagg}
\setlength{\headheight}{18pt}
\setlength{\headsep}{2pt}

\title{Stat 537 Homework 10}
\author{Andrea Mack and Kenny Flagg}
\date{April 28, 2016}

<<setup, echo=FALSE, message=FALSE, cache=FALSE>>=
require(knitr)
opts_chunk$set(echo = FALSE, comment = NA, width = 80,
               fig.align = 'center', fig.width = 6, fig.height = 3,
               size = 'footnotesize',
               dev = 'pdf', dev.args = list(pointsize = 8))
knit_theme$set('print')

require(xtable)
require(psych)
require(lavaan)
require(polycor)
require(psych)
require(semPlot)
require(effects)
require(rpart)
require(partykit)
require(openintro)
@

\begin{document}

\maketitle

{\it {\bf Part 1}: Classification Trees for the diagnosis of Mild Osteoarthritis using genetic information.}

{\it In Marshall et al. \verb|http://www.sciencedirect.com/science/article/pii/S1063458405001524|, they use logistic regression models to build and then validate (to some degree) a predictor for osteoarthritis/not using a set of genetic markers. Read the paper to answer the following questions.}

{\it Read the paper, focusing on the statistical aspects of their work.}

\begin{enumerate}

\item %1
{\it Fit their top ranked model from Supplemental Table 5. Make effects plots for the resulting estimated model using the following code.}

<<one>>=
marsh1 <- read.csv("marshalcomb.csv",header=T)
View(marsh1)
marsh1$D2 <- as.factor(marsh1$D2)
marsh1$Ethnicity <- as.factor(marsh1$Ethnicity)
marsh1R<-marsh1[,-c(1,2,3,4,5,6,8)]
@

<<one.plot,fig.height=12,fig.width=9,out.height='8in',out.width='6in'>>=
#Fit the model and call it glm1
#Top row in the excel file
glm1 <- glm(D2 ~ G2AN+IKBKAP+IL13RA1+LAMC1+MAFB+PF4+TNFAIP6, data=marsh1R, family=binomial)
summary(glm1)
plot(allEffects(glm1),type="link",rows=4,cols=2)
@

\pagebreak
\item %2
{\it They use logit values above or below 0 to predict whether an observation is diseased or not (below equation 1). Explain that choice.}

When the probability of getting the disease is the same as not getting the disease( p=0.5), the logit is equal to 0. So a logit $<$ zero is when there is a higher probability of not getting a disease, and when there is a higher probability of getting the disease than not getting the disease, logit is $>$ zero.

So if the probability of getting a disease is higher than half, logit is positive and we assign a subject to getting the disease. If the probability of getting a disease is less than half, logit is negative we assign a subject to not getting the disease.

\item %3
{\it In the section on page 865 ``Reference data set (AD1F2) for the best gene combinations" describe the type of modeling/model selection they are considering. Supplement table 5 contains more details of the results of this process.}

The statistician fit a model for each possible combination of predictor genes. He then made a ROC for each model, and computed the AUC. An AUC closer to 1 indicates that the model does a good job of correctly predicting mild OA and minimizes the misdiagnosis of mild OA.

The models with AUCs above 0.92, which would be classified as having ``Outstanding Discrimination", are bolded in the Supplemental Table 5, and the model with the AUC closest to 1 is chosen. Two models were tied for the highest AUC. The top model listed has fewer predictors than the second model (same AUC for the ROC), and we believe that is why it is chosen to be the top model.

\item %4
{\it Describe what they are doing in the section ``Prospective (Blind) test" starting on page 864. Use terminology from JWHT.}

They had an ensemble of 68 models that had an AUC of the ROC $>$ 0.9. For each of the models, a prediction was made to classify a person as mild OA or control. If a person is classified as mild OA, a 1 is assigned, if a person is classified as control, a -1 is assigned for each model. The sum of the 68 classifications is calculated and the majority vote from all models determines whether a person's class is mild OA or control.

\item %5
{\it Fit a classification tree using rpart using all 9 of the genetic variables listed below using {\texttt minbucket}=4 and {\texttt cp}=0.000000001 as in the code below. Prune the tree using the Min CV rule after consulting multiple calls to \texttt{ printcp()} and {\texttt plotcp()}. Do not load mvpart before doing this problem. Discuss the results of your CV process and how you chose your tree size. }

<<five, results='hide'>>=
set.seed(42362)
tree1<-rpart(factor(D2)~.,data=marsh1R,cp=0.000000001,minbucket=4)

#You can also see the Min CV choice in repeated calls using:
ret <- rep(NA, 100)
for (i in 1:100){
  tree1 <- rpart(factor(D2) ~ ., data = marsh1R, cp = 1e-09, minbucket = 4)
  a <- printcp(tree1)
  ret[i] <- min(which(a[, "xerror"] == min(a[, "xerror"])))
}
@

\item %6
{\it Based on these results, choose two different sized trees that could be reasonable based on the different CV selections. Report a plot of your two pruned classification trees using the partykit's plot(as.party(PRUNEDTREENAME)). Discuss the differences in the two trees.}


\item %7
{\it Adjust the following code as needed to make a plot of the predictions from the three models. Use these results to discuss the differences in these approaches/results. I had models called tree1p, tree2p, and glm1.}

<<seven>>=
fits<-data.frame(RPARTTREE1=predict(tree1p)[,2], RPARTTREE2=predict(tree2p)[,2],GLM1=predict(glm1,type="response"))
plot(fits)
@

\item %8
{\it The predictor variables were log10-transformed. Generally discuss how this impacts the tree-based approach vs the GLM approach. You can either undo their transformations and refit models or just discuss the impacts based on thinking about how the models work and why they might have done this transformation initially.}

{\bf Part 2 :} {\it Use the following code to identify an optimal predictive model for first year college GPAs. The following code will split out half the observations into a training data set and fit two different conditional inference trees using all available predictors. It also fits a recursive partitioning tree. }

\item %9
{\it ) Prune the tree using the 1SE and Min CV rules (either from a single 10-fold CV run or select a tree size for each as a consensus that you build from multiple CV runs). }

\item %10
{\it Calculate and compare the validation error for the 4 models using the withheld responses and discuss the choice of significance threshold and CV rule.}

<<ten>>=
data(satGPA)

set.seed(123456) #So that you can repeat the running of the code and get the same results
train=sample(1:1000,size=500)
ctree1<-ctree(FYGPA~.,data=satGPA[train,],mincriterion=0.95)
ctree2<-ctree(FYGPA~.,data=satGPA[train,],mincriterion=0.9)
rpart_full<-rpart(FYGPA~.,data=satGPA[train,],cp=0.00000001)
printcp(rpart2)
plotcp(rpart2)

#Do your own pruning and put results into rpart21SE and rpart2MinCV
predict(ctree1,newdata=satGPA[-train,])
predict(ctree2,newdata=satGPA[-train,])
predict(rpart21SE,newdata=satGPA[-train,])
predict(rpart2MinCV,newdata=satGPA[-train,])
@

\end{enumerate}

\pagebreak
\section*{R Code Appendix}

\subsection*{Problem 1}

<<one, echo=TRUE, eval=FALSE>>=
@
<<one.plot, echo=TRUE, eval=FALSE>>=
@


\subsection*{Problem 4}

\subsection*{Problem 5}

<<five, echo=TRUE, eval=FALSE>>=
@

\subsection*{Problem 6}

\subsection*{Problem 7}

<<seven, echo=TRUE, eval=FALSE>>=
@

\subsection*{Problem 8}

\subsection*{Problem 9}

\subsection*{Problem 10}

<<ten, echo=TRUE, eval=FALSE>>=
@

\end{document}
